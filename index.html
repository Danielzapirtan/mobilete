<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover" />
<title>Depth Room Scanner — WebXR (Depth-powered)</title>
<style>
  :root{--bg:#071021;--card:#081226;--accent:#4caf50;--muted:#9fb0c8}
  html,body{height:100%;margin:0;background:var(--bg);color:#e6f0ff;font-family:Inter,system-ui,Roboto,Segoe UI,Arial}
  #app{height:100%;display:flex;flex-direction:column}
  #viewer{flex:1;position:relative;overflow:hidden}
  canvas{width:100%;height:100%;display:block}
  .panel{position:absolute;left:12px;top:12px;z-index:60;background:linear-gradient(180deg, rgba(6,10,14,0.9), rgba(3,6,10,0.75));padding:12px;border-radius:10px;min-width:260px}
  .panel h3{margin:0 0 6px 0;color:var(--accent);font-size:15px}
  .panel button,.panel select{width:100%;padding:8px;margin-top:6px;border-radius:8px;border:0;background:#0b1622;color:#fff}
  #status{position:absolute;left:50%;transform:translateX(-50%);bottom:16px;background:rgba(0,0,0,0.6);padding:8px 12px;border-radius:20px;font-size:13px;z-index:60}
  #results{position:absolute;right:12px;top:12px;z-index:60;width:360px;max-height:72vh;overflow:auto;background:rgba(2,6,10,0.9);padding:12px;border-radius:10px;display:none}
  pre{white-space:pre-wrap;word-break:break-word;font-size:12px;background:#07101a;padding:10px;border-radius:6px;max-height:48vh;overflow:auto}
  .muted{font-size:12px;color:var(--muted)}
  #instructions{position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);z-index:80;background:rgba(2,6,10,0.95);padding:18px;border-radius:12px;max-width:480px}
  #instructions h2{margin:0 0 8px 0;color:var(--accent)}
  a.link{color:#8cc2ff;text-decoration:underline}
</style>
</head>
<body>
<div id="app">
  <div id="viewer"></div>

  <div class="panel" id="controls" style="display:none">
    <h3>Depth Room Scanner</h3>
    <div class="muted">Mode</div>
    <select id="mode">
      <option value="auto">Auto depth sampling</option>
      <option value="manual">Manual taps (hit-test fallback)</option>
    </select>
    <button id="startBtn">Start AR (request depth)</button>
    <button id="stopBtn" style="display:none">Stop AR</button>
    <button id="processBtn">Process & Generate JSON</button>
    <button id="resetBtn">Reset</button>
    <div class="muted" style="margin-top:8px;font-size:12px">Samples: <span id="sampleCount">0</span></div>
  </div>

  <div id="status" style="display:none">Ready</div>

  <div id="results" aria-live="polite">
    <div style="display:flex;align-items:center;justify-content:space-between">
      <strong style="color:var(--accent)">Empty Room JSON</strong>
      <div class="muted">Points: <span id="pointsCount">0</span></div>
    </div>
    <hr style="border:none;border-top:1px solid rgba(255,255,255,0.06);margin:8px 0" />
    <pre id="jsonOutput">No result yet.</pre>
    <div style="display:flex;gap:8px;margin-top:8px">
      <button id="copyBtn">Copy JSON</button>
      <button id="downloadBtn">Download</button>
      <button id="closeRes">Close</button>
    </div>
    <div class="muted" style="margin-top:8px">Tip: move slowly around the floor perimeter and sweep each wall. Avoid reflective surfaces.</div>
  </div>

  <div id="instructions">
    <h2>Depth-powered Room Scanner</h2>
    <p class="muted">This demo tries to use the WebXR Depth Sensing API. For best results on your Galaxy S21 Ultra: use Chrome (stable or Canary), allow camera permissions, and sweep slowly. If depth isn't available the app will fallback to hit-test sampling.</p>
    <div style="display:flex;gap:8px;margin-top:10px">
      <button id="okBtn">Let's scan — show controls</button>
      <a class="link" href="https://developer.mozilla.org/en-US/docs/Web/API/WebXR_Device_API" target="_blank" rel="noopener">WebXR docs</a>
    </div>
  </div>
</div>

<!-- Three.js -->
<script src="https://cdn.jsdelivr.net/npm/three@0.152.2/build/three.min.js"></script>

<script>
/*
 Depth Room Scanner (single-file)

 Approach:
  - Try to request WebXR session with 'depth-sensing' optional feature.
  - If available, each XRFrame we call getDepthInformation(view) to obtain XRDepthInformation.
  - Convert depth pixels -> camera-space rays -> world coordinates to build a point cloud (subsampled).
  - Use RANSAC to find dominant planes: floor (largest horizontal plane near lowest Y) and major vertical planes (walls).
  - Remove clusters likely to be furniture (small clusters above floor plane).
  - Fit polygons to inliers of wall planes by projecting to plane and computing convex hull (approx).
  - Output structured JSON.
  - If depth is not available, fallback to hit-test sampling (less accurate).

 Note: WebXR Depth Sensing API varies; we feature-detect carefully and fall back gracefully.
*/

(async function(){
  // UI refs
  const viewer = document.getElementById('viewer');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const processBtn = document.getElementById('processBtn');
  const resetBtn = document.getElementById('resetBtn');
  const controls = document.getElementById('controls');
  const statusEl = document.getElementById('status');
  const results = document.getElementById('results');
  const jsonOutput = document.getElementById('jsonOutput');
  const copyBtn = document.getElementById('copyBtn');
  const downloadBtn = document.getElementById('downloadBtn');
  const closeRes = document.getElementById('closeRes');
  const okBtn = document.getElementById('okBtn');
  const sampleCountEl = document.getElementById('sampleCount');
  const pointsCountEl = document.getElementById('pointsCount');
  const modeSelect = document.getElementById('mode');

  function showStatus(msg, show=true){
    statusEl.textContent = msg;
    statusEl.style.display = show ? 'block' : 'none';
    console.log('[depth-scanner]', msg);
  }

  // Three.js setup
  const scene = new THREE.Scene();
  const camera = new THREE.PerspectiveCamera(70, window.innerWidth/window.innerHeight, 0.01, 100);
  const renderer = new THREE.WebGLRenderer({alpha:true, antialias:true, preserveDrawingBuffer:true});
  renderer.setPixelRatio(window.devicePixelRatio);
  renderer.setSize(window.innerWidth, window.innerHeight);
  renderer.xr.enabled = true;
  viewer.appendChild(renderer.domElement);

  const light = new THREE.HemisphereLight(0xffffff, 0x444444, 1.0);
  scene.add(light);
  const vizGroup = new THREE.Group();
  scene.add(vizGroup);

  window.addEventListener('resize', ()=>{
    camera.aspect = window.innerWidth/window.innerHeight;
    camera.updateProjectionMatrix();
    renderer.setSize(window.innerWidth, window.innerHeight);
  });

  // Data
  let xrSession = null;
  let refSpace = null;
  let hitTestSource = null;
  let samples = []; // collected 3D points {x,y,z}
  let renderedPoints = []; // three.js meshes for markers
  let depthSupported = false;
  let usingDepth = false;

  // utility
  function addPointMarker(p,color=0x00ff88,size=0.02){
    const g = new THREE.SphereGeometry(size,8,8);
    const m = new THREE.MeshBasicMaterial({color});
    const s = new THREE.Mesh(g,m);
    s.position.set(p.x,p.y,p.z);
    vizGroup.add(s);
    renderedPoints.push(s);
  }
  function clearViz(){ while(vizGroup.children.length) vizGroup.remove(vizGroup.children[0]); renderedPoints=[]; }

  // Small sampling control (to avoid flooding)
  let lastDepthSampleTime = 0;

  // Feature detection: check navigator.xr and depth-sensing capability (best-effort)
  const xrAvailable = !!navigator.xr;
  if (!xrAvailable){
    document.getElementById('instructions').innerHTML += '<p style="color:#ffb86b">WebXR not supported in this browser. Use Chrome on Android.</p>';
    showStatus('WebXR not available', true);
  }

  // Basic RANSAC plane fitter (3D)
  function ransacPlane(points, iterations=500, threshold=0.03, minInliersRatio=0.05){
    // returns {normal:[nx,ny,nz], d, inliers: [indices]} or null
    if (points.length < 20) return null;
    let best = null;
    for (let it=0; it<iterations; it++){
      // pick 3 random non-collinear points
      const ia = Math.floor(Math.random()*points.length);
      let ib = Math.floor(Math.random()*points.length);
      let ic = Math.floor(Math.random()*points.length);
      if (ia===ib || ia===ic || ib===ic) continue;
      const A = points[ia], B = points[ib], C = points[ic];
      // compute normal = (B-A)x(C-A)
      const ux = B.x - A.x, uy = B.y - A.y, uz = B.z - A.z;
      const vx = C.x - A.x, vy = C.y - A.y, vz = C.z - A.z;
      const nx = uy*vz - uz*vy;
      const ny = uz*vx - ux*vz;
      const nz = ux*vy - uy*vx;
      const norm = Math.hypot(nx,ny,nz);
      if (norm < 1e-6) continue;
      const nnx = nx/norm, nny = ny/norm, nnz = nz/norm;
      // plane equation: nnx*(x-A.x) + nny*(y-A.y) + nnz*(z-A.z) = 0
      const d = -(nnx*A.x + nny*A.y + nnz*A.z);
      const inliers = [];
      for (let i=0;i<points.length;i++){
        const p = points[i];
        const dist = Math.abs(nnx*p.x + nny*p.y + nnz*p.z + d);
        if (dist < threshold) inliers.push(i);
      }
      if (!best || inliers.length > best.inliers.length){
        best = {normal:[nnx,nny,nnz], d, inliers};
        // early break if large plane
        if (inliers.length > points.length*0.6) break;
      }
    }
    if (!best) return null;
    if (best.inliers.length < Math.max(20, Math.floor(points.length*minInliersRatio))) return null;
    return best;
  }

  // Project points to plane coordinate system and compute 2D convex hull (Graham scan)
  function convexHull2D(points2D){
    // points2D: array of {x,y}
    if (points2D.length < 3) return points2D;
    // sort by x, then y
    points2D.sort((a,b)=> a.x===b.x ? a.y-b.y : a.x-b.x);
    const cross = (o,a,b) => (a.x - o.x)*(b.y - o.y) - (a.y - o.y)*(b.x - o.x);
    const lower = [];
    for (const p of points2D){
      while (lower.length >=2 && cross(lower[lower.length-2], lower[lower.length-1], p) <= 0) lower.pop();
      lower.push(p);
    }
    const upper = [];
    for (let i=points2D.length-1;i>=0;i--){
      const p = points2D[i];
      while (upper.length >=2 && cross(upper[upper.length-2], upper[upper.length-1], p) <= 0) upper.pop();
      upper.push(p);
    }
    upper.pop(); lower.pop();
    return lower.concat(upper);
  }

  // Helper: convert plane inliers to polygon in world coords
  function planeInliersToPolygon(points, plane){
    // plane.normal [nx,ny,nz], plane.d
    // choose axis: construct orthonormal basis u,v on plane
    const [nx,ny,nz] = plane.normal;
    // pick arbitrary vector not parallel to normal
    let ax = Math.abs(nx) > 0.9 ? [0,1,0] : [1,0,0];
    // u = normalize(normal x ax), v = normal x u
    const ux = ny*ax[2] - nz*ax[1];
    const uy = nz*ax[0] - nx*ax[2];
    const uz = nx*ax[1] - ny*ax[0];
    const ulen = Math.hypot(ux,uy,uz);
    if (ulen < 1e-6) return [];
    const u = [ux/ulen, uy/ulen, uz/ulen];
    const v = [ ny*u[2] - nz*u[1], nz*u[0] - nx*u[2], nx*u[1] - ny*u[0] ];
    // compute centroid on plane from inliers
    let cx=0,cy=0,cz=0;
    for(const idx of plane.inliers){
      cx += points[idx].x; cy += points[idx].y; cz += points[idx].z;
    }
    cx /= plane.inliers.length; cy /= plane.inliers.length; cz /= plane.inliers.length;
    // project each inlier onto u/v coords
    const pts2D = plane.inliers.map(i=>{
      const p = points[i];
      const rx = p.x - cx, ry = p.y - cy, rz = p.z - cz;
      return {x: rx*u[0] + ry*u[1] + rz*u[2], y: rx*v[0] + ry*v[1] + rz*v[2], world: p};
    });
    const hull = convexHull2D(pts2D);
    // convert hull back to world coords
    const poly = hull.map(h => {
      return {x: cx + h.x*u[0] + h.y*v[0], y: cy + h.x*u[1] + h.y*v[1], z: cz + h.x*u[2] + h.y*v[2]};
    });
    return poly;
  }

  // Remove furniture: cluster points (grid-based), remove small clusters above floor plane
  function removeFurniture(points, floorPlane){
    if (!points.length) return points;
    // simple grid hashing
    const cell = 0.25;
    const grid = new Map();
    for (let i=0;i<points.length;i++){
      const p = points[i];
      const key = `${Math.round(p.x/cell)}_${Math.round(p.y/cell)}_${Math.round(p.z/cell)}`;
      if (!grid.has(key)) grid.set(key, []);
      grid.get(key).push(i);
    }
    // cluster labels by flood across neighboring cells
    const visited = new Array(points.length).fill(false);
    const clusters = [];
    for (let i=0;i<points.length;i++){
      if (visited[i]) continue;
      // flood fill by proximity
      const stack=[i]; visited[i]=true; const cluster=[];
      while(stack.length){
        const idx = stack.pop(); cluster.push(idx);
        const p = points[idx];
        for (let j=0;j<points.length;j++){
          if (visited[j]) continue;
          if (Math.hypot(points[j].x-p.x, points[j].y-p.y, points[j].z-p.z) < 0.35){
            visited[j]=true; stack.push(j);
          }
        }
      }
      clusters.push(cluster);
    }
    // decide keepers: clusters that are large OR cluster centroid near floor plane
    const keepIdx = new Set();
    for (const c of clusters){
      let cx=0,cy=0,cz=0;
      for (const idx of c){ cx+=points[idx].x; cy+=points[idx].y; cz+=points[idx].z; }
      cx /= c.length; cy /= c.length; cz /= c.length;
      // distance to floor plane
      let floorDist = Infinity;
      if (floorPlane){
        const [nx,ny,nz] = floorPlane.normal;
        floorDist = Math.abs(nx*cx + ny*cy + nz*cz + floorPlane.d);
      }
      // decide
      const isSmall = c.length < 20;
      const isHigh = floorDist > 0.08 && cy -  (floorPlane ? (- (floorPlane.d + floorPlane.normal[0]*0 + floorPlane.normal[1]*0 + floorPlane.normal[2]*0) ) : 0) > 0.2;
      // keep cluster if not (small && high)
      if (!(isSmall && isHigh)){
        for(const idx of c) keepIdx.add(idx);
      }
    }
    const out = [];
    for (let i=0;i<points.length;i++) if (keepIdx.has(i)) out.push(points[i]);
    return out;
  }

  // Convert XRDepthInformation to point cloud for a given view pose
  function depthToPointCloud(depthInfo, viewPose, subsample=6){
    // depthInfo: XRCPUDepthInformation or similar. Implement tolerantly: check for .data, .rawValueToMeters, or .getFloat32/Uint16
    const pts = [];
    try {
      const width = depthInfo.width;
      const height = depthInfo.height;

      // The API differs; attempt to read depth data
      // If depthInfo.data exists and is TypedArray
      let depthArray = null;
      if (depthInfo.data) depthArray = depthInfo.data;
      else if (depthInfo.buffer) depthArray = new Uint16Array(depthInfo.buffer);
      else if (depthInfo instanceof ImageBitmap) {
        // unlikely; skip
        return pts;
      } else if (depthInfo.getDepth) {
        // unofficial helper
        depthArray = depthInfo.getDepth();
      }

      // Check for conversion function
      const rawToMeters = depthInfo.rawValueToMeters || depthInfo.rawValueToMetersFunction || (v => v); // fallback identity

      // projection params: use the view projection matrix to unproject
      // viewPose has transform.inverse? Instead use view projection matrix from XRView. We'll use THREE's camera matrix trick by constructing projection & view matrices.
      // But WebXR's view also includes projectionMatrix and transform.matrix in column-major float32Array.
      const proj = viewPose.projectionMatrix; // Float32Array(16)
      const viewMat = viewPose.transform.inverse ? viewPose.transform.inverse.matrix : null;
      // We'll use depth -> NDC -> camera space approach:
      // For pixel (i,j) depthRaw -> meters. NDC x = (i/width)*2 -1 ; y = (j/height)*2 -1 (note Y flipping sometimes)
      // Then unproject using inverse(proj) to get view-space direction.
      // We need inverse projection matrix:
      const projMat = new THREE.Matrix4().fromArray(proj);
      const invProj = new THREE.Matrix4().copy(projMat).invert();
      // For view-to-world, use viewPose.transform.matrix (4x4)
      const viewToWorld = new THREE.Matrix4().fromArray(viewPose.transform.matrix);

      // DepthArray could be float32, uint16 etc. We'll interpret generically.
      // iterate subsampled
      const stepX = Math.max(1, Math.floor(width/subsample));
      const stepY = Math.max(1, Math.floor(height/subsample));
      for (let y=0;y<height;y+=stepY){
        for (let x=0;x<width;x+=stepX){
          const idx = y*width + x;
          let raw = depthArray ? depthArray[idx] : undefined;
          if (raw === undefined) continue;
          const meters = (typeof raw === 'number' ? raw : raw[0]) * (rawToMeters ? rawToMeters : 1);
          if (!isFinite(meters) || meters <= 0) continue;
          // NDC
          const ndcX = (x / (width-1)) * 2 - 1;
          const ndcY = -((y / (height-1)) * 2 - 1); // flip Y to NDC top-left -> bottom-left
          // point in clip space
          const clip = new THREE.Vector4(ndcX, ndcY, -1, 1);
          // convert to view-space direction
          clip.applyMatrix4(invProj);
          // perspective divide
          if (clip.w === 0) continue;
          clip.divideScalar(clip.w);
          // this gives a ray direction in view-space. Normalize and scale to distance meters
          const dir = new THREE.Vector3(clip.x, clip.y, clip.z).normalize();
          // convert view-space point at distance meters
          const localPoint = new THREE.Vector4(dir.x*meters, dir.y*meters, dir.z*meters, 1.0);
          // transform to world space
          localPoint.applyMatrix4(viewToWorld);
          pts.push({x: localPoint.x, y: localPoint.y, z: localPoint.z});
        }
      }
    } catch(e){
      console.warn('depthToPointCloud failed', e);
    }
    return pts;
  }

  // XR Frame loop when depth is enabled
  function onXRFrameDepth(time, frame){
    if (!xrSession) return;
    const pose = frame.getViewerPose(refSpace);
    if (!pose) return;
    // sample first view
    const view = pose.views[0];
    // get depth info (API name may vary)
    let depthInfo = null;
    try {
      // Preferred modern API: frame.getDepthInformation(view)
      if (frame.getDepthInformation) depthInfo = frame.getDepthInformation(view);
      // Some implementations might attach depth to view: view.depthInformation
      if (!depthInfo && view && view.depthInformation) depthInfo = view.depthInformation;
      // Or to frame: frame.depth
      if (!depthInfo && frame.depth) depthInfo = frame.depth;
    } catch(e){
      depthInfo = null;
    }
    if (depthInfo){
      usingDepth = true;
      // limit sampling rate for performance
      const now = performance.now();
      if (now - lastDepthSampleTime > 400){ // sample ~2-3 fps
        lastDepthSampleTime = now;
        const pts = depthToPointCloud(depthInfo, view, 160); // larger subsample param reduces points (tune)
        for(const p of pts){
          samples.push(p);
          // Optionally add marker (but don't add thousands)
          if (samples.length % 50 === 0) addPointMarker(p,0x66bb6a,0.02);
        }
        sampleCountEl.textContent = samples.length;
      }
    } else {
      // no depth: optionally do hit-test sampling if hitTestSource available
      // we'll keep hit-test sampling separated
    }

    renderer.render(scene, camera);
  }

  // XR Frame loop for hit-test fallback
  function onXRFrameHit(time, frame){
    if (!xrSession) return;
    const pose = frame.getViewerPose(refSpace);
    if (!pose) return;
    // hit test
    if (hitTestSource){
      const hitResults = frame.getHitTestResults(hitTestSource);
      if (hitResults.length){
        const hit = hitResults[0];
        const hp = hit.getPose(refSpace);
        if (hp){
          samples.push({x:hp.transform.position.x, y:hp.transform.position.y, z:hp.transform.position.z});
          if (samples.length % 8 === 0) addPointMarker(samples[samples.length-1], 0x66bb6a, 0.02);
          sampleCountEl.textContent = samples.length;
        }
      }
    }
    renderer.render(scene, camera);
  }

  // start XR session with depth-sensing request
  async function startXR(){
    if (!navigator.xr) { alert('WebXR not available'); return; }
    // prefer depth-sensing usage preferences; different browsers accept different shapes — be permissive
    const depthOptions = {
      usagePreference: ['cpu-optimized','gpu-optimized'],
      dataFormatPreference: ['luminance-alpha','float32','float32-external'] // tolerant
    };
    const sessionInit = {
      optionalFeatures: ['dom-overlay','hit-test','anchors','depth-sensing'],
      requiredFeatures: ['local'],
      domOverlay: { root: document.body }
    };
    // Some implementations accept depth-sensing nested object in requestSession
    try {
      xrSession = await navigator.xr.requestSession('immersive-ar', Object.assign({}, sessionInit, {depthSensing: depthOptions}));
    } catch (e){
      console.warn('requestSession with depthSensing failed, trying without depth param', e);
      try {
        xrSession = await navigator.xr.requestSession('immersive-ar', sessionInit);
      } catch (err){
        alert('Failed to start AR session: ' + (err && err.message ? err.message : err));
        showStatus('AR start failed', true);
        return;
      }
    }

    // set renderer session
    renderer.xr.setSession(xrSession);
    refSpace = await xrSession.requestReferenceSpace('local');

    // try hit-test source (fallback)
    try {
      const viewerSpace = await xrSession.requestReferenceSpace('viewer');
      hitTestSource = await xrSession.requestHitTestSource({space: viewerSpace});
    } catch(e){
      hitTestSource = null;
      console.warn('hit test not available', e);
    }

    // detect if depth-sensing is actually available by attempting to get depth info on the next frame
    depthSupported = false;
    usingDepth = false;

    // set animation loop and a pre-frame probe for depth availability
    renderer.setAnimationLoop((t,frame) => {
      const pose = frame.getViewerPose(refSpace);
      if (!pose) {
        renderer.render(scene, camera);
        return;
      }
      // probe for depth once
      if (!depthSupported){
        try {
          const view = pose.views[0];
          let depthInfo = null;
          if (frame.getDepthInformation) depthInfo = frame.getDepthInformation(view);
          if (!depthInfo && view && view.depthInformation) depthInfo = view.depthInformation;
          if (depthInfo){
            console.log('Depth information available');
            depthSupported = true;
          } else {
            // depth not available (yet)
            depthSupported = false;
          }
        } catch(e){
          depthSupported = false;
        }
      }
      // pick loop
      if (depthSupported && modeSelect.value === 'auto'){
        onXRFrameDepth(t, frame);
      } else {
        onXRFrameHit(t, frame);
      }
    });

    xrSession.addEventListener('end', ()=>{
      renderer.setAnimationLoop(null);
      xrSession = null;
      showStatus('AR session ended', true);
      startBtn.style.display = 'inline-block';
      stopBtn.style.display = 'none';
    });

    controls.style.display = 'block';
    startBtn.style.display = 'none';
    stopBtn.style.display = 'inline-block';
    showStatus('AR session started — scanning...');
  }

  async function stopXR(){
    if (xrSession) await xrSession.end();
    renderer.setAnimationLoop(null);
    xrSession = null;
    showStatus('AR stopped', false);
  }

  // Process collected samples into planes + JSON
  function processAndExport(){
    if (!samples.length){
      alert('No samples collected yet. Walk around and sweep the room.');
      return;
    }
    showStatus('Processing samples (this may take a few seconds)...');
    // copy points
    let pts = samples.map(p=>({x:p.x,y:p.y,z:p.z}));
    // downsample for heavy operations
    if (pts.length > 8000){
      const step = Math.ceil(pts.length / 6000);
      pts = pts.filter((_,i)=> i%step === 0);
    }

    // First, detect dominant plane (try to detect floor: horizontal plane with largest area near lowest y)
    const planes = [];
    let leftovers = pts.slice();
    for (let i=0;i<3;i++){ // try to find up to 3 major planes
      const plane = ransacPlane(leftovers, 800, 0.04, 0.02);
      if (!plane) break;
      planes.push(plane);
      // remove inliers from leftovers
      const newLeft = [];
      const inlierSet = new Set(plane.inliers);
      for (let j=0;j<leftovers.length;j++){
        if (!inlierSet.has(j)) newLeft.push(leftovers[j]);
      }
      leftovers = newLeft;
    }

    // Classify which plane is floor: choose plane whose normal is most aligned with +Y (vertical axis) and has low centroid Y
    function planeCentroid(points, inliers){
      let cx=0,cy=0,cz=0;
      for (const i of inliers){ cx+=points[i].x; cy+=points[i].y; cz+=points[i].z; }
      return {x:cx/inliers.length, y:cy/inliers.length, z:cz/inliers.length};
    }

    let floorPlane = null;
    const candidatePlanes = [];
    for (const pl of planes){
      const centroid = planeCentroid(pts, pl.inliers);
      const nx = pl.normal[0], ny = pl.normal[1], nz = pl.normal[2];
      const verticalScore = Math.abs(ny); // 1 if perfectly horizontal
      candidatePlanes.push({pl, centroid, verticalScore, areaEstimate: pl.inliers.length});
    }
    if (candidatePlanes.length){
      // prefer high verticalScore and low centroid y
      candidatePlanes.sort((a,b) => (b.verticalScore - a.verticalScore) || (a.centroid.y - b.centroid.y));
      floorPlane = candidatePlanes[0].pl;
    }

    // Remove furniture clusters using floorPlane
    const cleaned = removeFurniture(pts, floorPlane);

    // Recompute planes on cleaned set for walls
    const wallPlanes = [];
    let leftover2 = cleaned.slice();
    for (let i=0;i<4;i++){
      const p = ransacPlane(leftover2, 800, 0.04, 0.02);
      if (!p) break;
      // discard horizontal plane (floor) by checking normal
      if (Math.abs(Math.abs(p.normal[1]) - 1) < 0.2){
        // horizontal -> probably floor/ceiling, skip here
        // remove its inliers anyway
        const inS = new Set(p.inliers);
        leftover2 = leftover2.filter((_,idx)=>!inS.has(idx));
        continue;
      }
      // store wall plane
      wallPlanes.push(p);
      // remove inliers
      const inS = new Set(p.inliers);
      leftover2 = leftover2.filter((_,idx)=>!inS.has(idx));
    }

    // Build polygons for wall planes
    const walls = [];
    for (const wp of wallPlanes){
      const poly = planeInliersToPolygon(cleaned, wp);
      if (poly && poly.length >= 3){
        // compute normal (ensure outward direction)
        walls.push({normal: wp.normal, polygon: poly});
      }
    }

    // Build floor polygon from floorPlane inliers
    let floorPolygon = null;
    if (floorPlane){
      const floorPoly = planeInliersToPolygon(pts, floorPlane);
      if (floorPoly && floorPoly.length >= 3) floorPolygon = floorPoly;
    } else {
      // fallback: compute convex hull of cleaned points projected to XZ
      const pts2D = cleaned.map(p=>({x:p.x,y:p.z}));
      const hull2D = convexHull2D(pts2D);
      // convert to 3D polygon at minY
      if (hull2D.length >= 3){
        const minY = Math.min(...cleaned.map(p=>p.y));
        floorPolygon = hull2D.map(h => ({x:h.x,y:minY,z:h.y}));
      }
    }

    // Compute room dimensions: bounding box of floor polygon
    let width=0, depth=0, heightEst=2.7;
    if (floorPolygon){
      const xs = floorPolygon.map(p=>p.x);
      const zs = floorPolygon.map(p=>p.z);
      width = Math.abs(Math.max(...xs) - Math.min(...xs));
      depth = Math.abs(Math.max(...zs) - Math.min(...zs));
      // estimate height using highest cleaned point above floor
      const floorY = Math.min(...floorPolygon.map(p=>p.y));
      const highY = Math.max(...cleaned.map(p=>p.y));
      heightEst = Math.round((highY - floorY)*100)/100;
      if (!isFinite(heightEst) || heightEst <= 1.6) heightEst = 2.7;
    }

    // Prepare JSON
    const generatedAt = new Date().toISOString();
    const metadata = {version:'1.0', generatedAt, source:'webxr-depth-room-scanner', unit:'meters', deviceHint:'Galaxy S21-like (depth capable)'};
    const json = {
      metadata,
      dimensions: { width: parseFloat(width.toFixed(2)), depth: parseFloat(depth.toFixed(2)), height: parseFloat(heightEst.toFixed(2)), area: parseFloat((width*depth).toFixed(2)), volume: parseFloat((width*depth*heightEst).toFixed(2)) },
      floor: floorPolygon ? { vertices: floorPolygon } : null,
      walls: walls.map((w,idx) => ({ id:`wall_${idx+1}`, normal: w.normal, polygon: w.polygon })),
      samples: cleaned.length,
      rawPointsCount: pts.length,
      notes: 'Planes detected via RANSAC. Furniture heuristically removed by clustering above floor plane. This is an approximation.'
    };

    // visualize outputs: clear & draw floor polygon + wall lines
    clearViz();
    if (floorPolygon){
      // draw small points on polygon
      for (const v of floorPolygon.slice(0,200)){
        addPointMarker(v, 0x44ff88, 0.015);
      }
    }
    // wall visualization
    for (const w of walls){
      for (const v of (w.polygon||[]).slice(0,200)) addPointMarker(v, 0x66ccff, 0.018);
    }

    // show JSON
    jsonOutput.textContent = JSON.stringify(json, null, 2);
    results.style.display = 'block';
    pointsCountEl.textContent = cleaned.length;
    showStatus('Processing complete — review JSON', true);
  }

  // UI wiring
  okBtn.addEventListener('click', ()=>{
    document.getElementById('instructions').style.display='none';
    controls.style.display = 'block';
  });
  startBtn.addEventListener('click', async ()=> {
    await startXR();
  });
  stopBtn.addEventListener('click', async ()=> {
    await stopXR();
  });
  resetBtn.addEventListener('click', ()=>{
    samples = [];
    clearViz();
    jsonOutput.textContent = 'No result yet.';
    results.style.display = 'none';
    sampleCountEl.textContent = samples.length;
    pointsCountEl.textContent = 0;
    showStatus('Reset — ready', true);
  });

  processBtn.addEventListener('click', ()=> processAndExport());

  closeRes.addEventListener('click', ()=> results.style.display = 'none');

  copyBtn.addEventListener('click', async ()=>{
    try {
      await navigator.clipboard.writeText(jsonOutput.textContent);
      showStatus('JSON copied to clipboard', true);
    } catch(e){ alert('Copy failed: ' + e); }
  });

  downloadBtn.addEventListener('click', ()=>{
    const txt = jsonOutput.textContent;
    if (!txt || txt.trim() === 'No result yet.') { alert('Nothing to download'); return; }
    const blob = new Blob([txt], {type: 'application/json'});
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url; a.download = 'empty_room_depth.json'; document.body.appendChild(a); a.click(); a.remove(); URL.revokeObjectURL(url);
    showStatus('JSON downloaded', true);
  });

  // expose manual tap sampling: when user taps screen in manual mode, create a hit-test sample
  renderer.domElement.addEventListener('click', (ev)=>{
    if (!xrSession) return;
    if (modeSelect.value !== 'manual') return;
    // fire a one-off transient hit-test if frame available is not accessible here; as fallback, use existing hitTestSource result stored by frame loop
    // user can also tap to note positions but for robust manual sampling you need devices that implement transient input.
    showStatus('Manual sampling is approximated in this demo. Prefer auto depth mode for best results.', true);
  });

  // initial UI
  controls.style.display = 'none';
  showStatus('Ready — dismiss the instructions to show controls', true);
})();
</script>
</body>
</html>